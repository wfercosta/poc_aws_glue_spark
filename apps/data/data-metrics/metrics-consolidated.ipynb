{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-------+--------------------+--------+------+-------------+-------------------+\n",
      "|      date|     context|  family|version|            resource|priority|status|response_time|          timestamp|\n",
      "+----------+------------+--------+-------+--------------------+--------+------+-------------+-------------------+\n",
      "|2022-02-17|open-banking|accounts|     v1|/accounts/v1/acco...|  MEDIUM|   500|         1000|2022-02-17 13:50:36|\n",
      "|2022-02-17|open-banking|accounts|     v1|/accounts/v1/acco...|  MEDIUM|   500|         1000|2022-02-17 13:51:36|\n",
      "|2022-02-17|open-banking|accounts|     v1|/accounts/v1/acco...|  MEDIUM|   429|         1000|2022-02-17 13:52:36|\n",
      "|2022-02-17|open-banking|accounts|     v1|/accounts/v1/acco...|  MEDIUM|   429|         1000|2022-02-17 13:53:36|\n",
      "+----------+------------+--------+-------+--------------------+--------+------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- context: string (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- resource: string (nullable = true)\n",
      " |-- priority: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- response_time: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "gc = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "ddf = gc.create_dynamic_frame_from_options(\"s3\"\\\n",
    "                                          , {\"paths\": [\"s3://wfercosta-spark/logs/DAILY_20211125.csv\"]}\\\n",
    "                                          ,\"csv\"\\\n",
    "                                          ,{'withHeader':True})\n",
    "        \n",
    "df = ddf.toDF()\n",
    "df = df.select(['date', 'context', 'family', 'version', 'resource'\\\n",
    "                , 'priority', 'status', 'response_time', 'timestamp'])\n",
    "\n",
    "df = df.withColumn('date', F.to_date(df.date))\n",
    "df = df.withColumn('timestamp', F.to_timestamp(df.timestamp))\n",
    "df = df.withColumn('response_time', df.response_time.cast('int'))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dd0c6b20bb434a98d2cb21eae37e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For each endpoint in day and by context\n",
    "# date, context, family, version, resource, priority, total_downtime_sec, total_uptime_rate\n",
    "\n",
    "df_aval = df\n",
    "\n",
    "column_list = ['date', 'context', 'resource']\n",
    "\n",
    "\n",
    "# Find distribution of data\n",
    "count_if = lambda condition: F.sum(F.when(condition, 1).otherwise(0))\n",
    "\n",
    "df_aval_class = df_aval.groupby('date', 'context', 'resource')\\\n",
    "                .agg( \\\n",
    "                   F.sum(F.lit(1)).alias('nr_total') \\\n",
    "                 , count_if(F.col('status') < 500).alias('nr_success'))\n",
    "\n",
    "\n",
    "df_aval_class = df_aval_class.withColumn('distribution', \\\n",
    "                                         F.when(df_aval_class.nr_total - df_aval_class.nr_success == 0, 'SUCCESS')\\\n",
    "                                         .when(df_aval_class.nr_total - df_aval_class.nr_success == df_aval_class.nr_total, 'ERROR')\n",
    "                                         .otherwise('MIXED'))\n",
    "\n",
    "df_aval = df_aval.join(df_aval_class, ['date', 'context', 'resource'], 'outer').drop('nr_total', 'nr_success')\n",
    "\n",
    "\n",
    "#Process distribution ERROR\n",
    "window = Window().partitionBy([F.col(x) for x in column_list]).orderBy([\\\n",
    "                                                                        F.col('resource')\\\n",
    "                                                                        , F.col('timestamp').desc()])\n",
    "\n",
    "df_aval_error = df_aval.filter(df_aval.distribution == 'ERROR')\n",
    "\n",
    "df_aval_error = df_aval_error.withColumn('row', F.row_number().over(window))\\\n",
    "    .filter(F.col('row') == 1)\n",
    "\n",
    "\n",
    "df_aval_error = df_aval_error.withColumn('timestamp_', F.date_trunc('day', F.col('timestamp')))\n",
    "df_aval_error = df_aval_error.withColumn('total_downtime_sec', F.col('timestamp').cast('long') - F.col('timestamp_').cast('long'))\n",
    "df_aval_error = df_aval_error.withColumn('total_uptime_rate', F.lit(0))\n",
    "df_aval_error = df_aval_error.drop('row', 'timestamp', 'timestamp_', 'response_time', 'distribution', 'status')\n",
    "\n",
    "#Process distribution SUCCESS\n",
    "\n",
    "df_aval_success = df_aval.filter(df_aval.distribution == 'SUCCESS')\\\n",
    "                            .select('date','context','family','version','resource','priority')\\\n",
    "                            .distinct()\n",
    "\n",
    "df_aval_success = df_aval_success.withColumn('total_uptime_rate', F.lit(1))\n",
    "df_aval_success = df_aval_success.withColumn('total_downtime_sec', F.lit(0))\n",
    "\n",
    "#Process distribution MIXED\n",
    "\n",
    "#Replicates the columns unneeded and replicates prev rows values on next\n",
    "\n",
    "window = Window().partitionBy([F.col(x) for x in column_list]).orderBy([\\\n",
    "                                                                        F.col('resource')\\\n",
    "                                                                        , F.col('timestamp')])\n",
    "\n",
    "\n",
    "df_aval_mixed = df_aval.filter(df_aval.distribution == 'MIXED')\n",
    "\n",
    "df_aval_mixed = df_aval_mixed.withColumn('state', F.when(df_aval_mixed.status >= 500, 'ERROR').otherwise('SUCCESS'))\n",
    "\n",
    "df_aval_mixed = df_aval_mixed.withColumn('state_prev', F.lag('state').over(window))\n",
    "\n",
    "#Filters intermediate row that is not a state transition\n",
    "df_aval_mixed = df_aval_mixed.filter(df_aval_mixed.state_prev.isNull() \\\n",
    "                                | (df_aval_mixed.state_prev != df_aval_mixed.state))\n",
    "\n",
    "df_aval_mixed = df_aval_mixed.withColumn(\"timestamp_prev\", F.lag(\"timestamp\").over(window))\n",
    "df_aval_mixed = df_aval_mixed.withColumn(\"timestamp_prev\", F.coalesce(df_aval_mixed.timestamp_prev, df_aval_mixed.timestamp)) \n",
    "\n",
    "df_aval_mixed = df_aval_mixed.filter(df_aval_mixed.state_prev.isNull() \\\n",
    "                         | (df_aval_mixed.state == 'SUCCESS'))\n",
    "\n",
    "\n",
    "# Calculates the downtime in seconds\n",
    "df_aval_mixed = df_aval_mixed.withColumn('total_downtime_sec'\\\n",
    "                             , F.col('timestamp').cast('long') - F.col('timestamp_prev').cast('long'))\n",
    "\n",
    "df_aval_mixed = df_aval_mixed.groupby(['date', 'context'\\\n",
    "                , 'family', 'version'\\\n",
    "                , 'resource', 'priority'])\\\n",
    "                    .agg(\\\n",
    "                         F.sum('total_downtime_sec').alias('total_downtime_sec'))\n",
    "\n",
    "\n",
    "# Calculates the uptime rate \n",
    "calculate_uptime_rate = lambda downtime_sec: (((24 * 60 * 60) - downtime_sec)/(24 * 60 * 60))\n",
    "\n",
    "df_aval_mixed = df_aval_mixed.withColumn('total_uptime_rate', calculate_uptime_rate(df_aval_mixed.total_downtime_sec))\n",
    "df_aval_mixed = df_aval_mixed.withColumn('total_uptime_rate', F.round(df_aval_mixed.total_uptime_rate, 3))\n",
    "\n",
    "\n",
    "selected_columns = ['date', 'context', 'family', 'version', 'resource'\\\n",
    "                    , 'priority', 'total_downtime_sec', 'total_uptime_rate']\n",
    "\n",
    "df_aval_mixed = df_aval_mixed.select(selected_columns)\n",
    "df_aval_error = df_aval_error.select(selected_columns)\n",
    "df_aval_success = df_aval_success.select(selected_columns)\n",
    "\n",
    "df_aval = df_aval_mixed.union(df_aval_error)\n",
    "df_aval = df_aval.union(df_aval_success)\n",
    "\n",
    "\n",
    "df_aval.show(truncate=False)\n",
    "df_aval.printSchema()\n",
    "\n",
    "print(json.dumps(df_aval.toJSON().map(lambda j: json.loads(j)).collect(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General in a day and context\n",
    "# date, context, avg_tps, peak_tps, total_nr_rejections, total_nr_errors, total_uptime_rate, total_downtime_sec, total_scheduled_outage\n",
    "\n",
    "df_gen = df\n",
    "\n",
    "# Calculates the right time interval for each row\n",
    "calculate_interval = lambda field: (F.round(field.cast('long') / 60) * 60.0)\\\n",
    "                        .cast(\"timestamp\")\n",
    "\n",
    "df_gen = df_gen.withColumn('timestamp_intvl_1_min', calculate_interval(df_gen.timestamp))\n",
    "\n",
    "\n",
    "count_if = lambda condition: F.sum(F.when(condition, 1).otherwise(0))\n",
    "\n",
    "\n",
    "df_gen = df_gen.groupby(['date', 'context', 'timestamp_intvl_1_min'])\\\n",
    "            .agg(\\\n",
    "                 F.count(F.lit(1)).alias('tpm')\\\n",
    "                 , count_if(F.col('status') == 429).alias('total_nr_rejections') \\\n",
    "                 , count_if(F.col('status') >= 500).alias('total_nr_errors'))\n",
    "\n",
    "\n",
    "df_gen = df_gen.withColumn('avg_tps', F.round(df_gen.tpm / 60, 3))\n",
    "\n",
    "\n",
    "df_gen = df_gen.groupby(['date', 'context'])\\\n",
    "            .agg(\\\n",
    "                   F.round(F.avg('avg_tps'), 3).alias('avg_tps')\\\n",
    "                 , F.round(F.max('avg_tps'), 3).alias('peak_tps')\\\n",
    "                 , F.sum('total_nr_rejections').alias('total_nr_rejections')\\\n",
    "                 , F.sum('total_nr_errors').alias('total_nr_errors'))\n",
    "\n",
    "\n",
    "df_gen_aval = df_aval.groupby(['date', 'context'])\\\n",
    "            .agg(\\\n",
    "                  F.sum('total_downtime_sec').alias('total_downtime_sec')\\\n",
    "                , F.sum('total_uptime_rate').alias('total_uptime_rate'))\n",
    "\n",
    "\n",
    "df_gen = df_gen.join(df_gen_aval, ['date', 'context'])\n",
    "\n",
    "\n",
    "df_gen.show()\n",
    "df_gen.printSchema()\n",
    "\n",
    "print(json.dumps(df_gen.toJSON().map(lambda j: json.loads(j)).collect(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General in a day and by Priority and context\n",
    "# date, context, priority, total_nr_invocations, avg_response\n",
    "\n",
    "df_gen_pri = df.groupby(['date', 'context', 'priority'])\\\n",
    "                .agg(\\\n",
    "                       F.count(F.lit(1)).alias('total_nr_invocations')\\\n",
    "                     , F.avg('response_time').alias('avg_response'))\n",
    "\n",
    "df_gen_pri.show()\n",
    "df_gen_pri.printSchema()\n",
    "\n",
    "\n",
    "print(json.dumps(df_gen_pri.toJSON().map(lambda j: json.loads(j)).collect(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
